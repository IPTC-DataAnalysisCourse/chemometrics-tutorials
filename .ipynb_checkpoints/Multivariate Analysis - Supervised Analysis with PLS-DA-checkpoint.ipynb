{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Analysis - PLS-DA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will perform a supervised *multivariate* PLS-DA analysis of the *[AddNeuroMed](https://zenodo.org/doi/10.5281/zenodo.4053166)* dataset of Alzheimer's disease. It is recommended to finish first the notebook *Multivariate Analysis - PCA*.\n",
    "This is LC-MS based version of the tutorial, for NMR version please visit the course [GitHub Repository](https://github.com/orgs/IPTC-DataAnalysisCourse/repositories).\n",
    "\n",
    "The notebook is divided in the following steps:\n",
    "\n",
    "1) Model fitting basics: Fit PLS-DA models to predict sex from the metabolic profile data, using different types of scaling.\n",
    "\n",
    "2) Model cross-validation and component selection: Describe model cross-validation, parameter selection and performance assessment, including permutation testing.\n",
    "\n",
    "3) Model interpretation: Describe some of the available variable importance metrics for PLS-DA, and highlight which variables might be important for the discrimination. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code import\n",
    "\n",
    "Import all the packages and configure notebook plotting mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required python packages including \n",
    "# the custom pyChemometric Model objects\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "\n",
    "from pyChemometrics.ChemometricsPLSDA import ChemometricsPLSDA\n",
    "from pyChemometrics.ChemometricsScaler import ChemometricsScaler\n",
    "from pyChemometrics.ChemometricsOrthogonalPLSDA import ChemometricsOrthogonalPLSDA\n",
    "from pyChemometrics.plotting_utils import _scatterplots\n",
    "\n",
    "# Use to obtain same values as in the text\n",
    "np.random.seed(350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data conversion warnings to appear only once to avoid repetition during CV\n",
    "warnings.filterwarnings(\"ignore\", category=DataConversionWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "# I think we don't need this anymore - Need to check (flsoares232)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell sets up the figure display mode. The *notebook* mode allows interactive plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the plot backend to support interactive plotting - it prolongs the time to render the plots, NOT RECOMMENDED IN CLOAB\n",
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We will now import the LC-MS data with the metadata (Y variables) and feature annotation for LC-MS.\n",
    "\n",
    "Then we split the metadata into two parts: \n",
    "\n",
    "rpos_x_matrix - LC-MS data matrix\n",
    "\n",
    "gender_y - Metadata that will act as the response variable for the PLS-DA model (sex/gender in this instance)\n",
    "\n",
    "retention_times, mz_values - annotation for the features of the rpos_x_matrix data \n",
    "\n",
    "#### Metadata\n",
    "Y - represents the sex of the individudals (0: Female, 1: Male) used as the response variable for the PLS-DA model\n",
    "\n",
    "##### NB - Full data available from: [https://zenodo.org/doi/10.5281/zenodo.4053166](https://zenodo.org/doi/10.5281/zenodo.4053166)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dementia_rpos_dataset = pd.read_csv(\"./data/Dementia U RPOS_combinedData.csv\",delimiter=',')\n",
    "\n",
    "# Delete samples where outcome variable is unknown (In this example QC samples)\n",
    "dementia_rpos_dataset = dementia_rpos_dataset[~dementia_rpos_dataset['Gender'].isnull()]\n",
    "\n",
    "x_data = dementia_rpos_dataset.iloc[:, 13::].values\n",
    "\n",
    "variable_names = dementia_rpos_dataset.columns[13::]\n",
    "\n",
    "# Use pandas Categorical type\n",
    "gender_y = pd.Categorical(dementia_rpos_dataset['Gender']).codes\n",
    "\n",
    "# Extract the retention times and m/z to use in 2D plots of the dataset\n",
    "retention_times = np.array([x.split('_')[0] for x in variable_names], dtype='float')/60\n",
    "mz_values = np.array([x.split('_')[1][0:-3] for x in variable_names], dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Compare binary response to labels\n",
    "print(dementia_rpos_dataset['Gender'].value_counts())\n",
    "print(pd.DataFrame(gender_y).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Split data into train test\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, gender_y, test_size=0.10, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: To apply the analyses exemplified in this notebook to any other dataset, just modify the cell above to import the data matrices and vectors X and Y from any other source file. You can also use this methods for analysis of NMR data but we recommend to run NMR version of this code available through the course's [GitHub Repository](https://github.com/orgs/IPTC-DataAnalysisCourse/repositories). \n",
    "\n",
    "The expected data types and formatting for **X** and **Y** are:\n",
    "\n",
    "   **X**: Any data matrix with n rows (observations/samples) and p columns (variables/features). The matrix should be provided as a [numpy.ndarray](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html) object, with 2 dimensions, and with shape = (n, p). We recommend using the *numpy* function [numpy.genfromtxt](https://numpy.org/devdocs/reference/generated/numpy.genfromtxt.html) or the *pandas* [pandas.read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) function to read the data from a text file. When using the *pandas.read_csv* function, extract the data matrix as a *numpy.ndarray* from the pandas.DataFrame object using the `.values` attribute. \n",
    "```\n",
    "X_DataFrame = pd.read_csv(\"./data/X_spectra.csv\")\n",
    "X = X_DataFrame.values\n",
    "```\n",
    "   \n",
    "   **Y** vectors: Each **Y** vector should be a 1-dimensional [numpy.ndarray](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html) object, with a number and ordering of elements matching the rows in **X**. For continuous variables, any regular *numpy.ndarray* with a data type of `int` (integers only) or `float` can be used.\n",
    "   ```\n",
    "   Y_continuous = numpy.ndarray([23.4, 24, 0.3, -1.23], dtype='float')\n",
    "   ```\n",
    "To encode binary class labels, a *numpy.ndarray* of dtype `int`, with 0 and 1 as labels (e.g., 0 = Control, 1 = Case) must be used. The way in which classes are encoded will affect the model interpretation: the class labeled as 1 is used as the \"positive/case\" class by the *pyChemometrics* objects.\n",
    "   \n",
    "   In the example above, we used the *pandas* [Categorical](https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html) datatype to handle the conversion of the original numerical values (1, 2) to the required (0, 1) labels. After converting a column to a `Categorical` datatype, the `.codes` attribute returns a vector with the same length of the original Y, but where each value is replaced by their integer (`int`) code. The correspondence between code and category can be inspected with the `categories` attribute. The order of the labels in `.codes` is the same as the order of the `categories` attribute (i.e. 0 is the first element in `categories`, 1 the second and so on).\n",
    "   ```\n",
    "   Y1 = pd.Categorical(Y.iloc[:, 1])\n",
    "   Y1.codes # The numerical label\n",
    "   Y1.categories # Original text or numerical description of the category\n",
    "   ```\n",
    "   [get_dummies](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) is another helpful function to perform dummy (0-1) encoding of variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot all the peaks in the dataset coloured by mean intesity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the spectra in the dataset\n",
    "_scatterplots(np.log(np.mean(x_data, axis=0) + 1), xaxis=retention_times, yaxis=mz_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLS-DA modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Model fitting basics\n",
    "\n",
    "In this section we will fit a PLS-DA model to classify *C.elegans* samples based on their genotype, and assess the metabolic differences between *sod-2* mutants and the parent wild-type (N2).\n",
    "\n",
    "As an example, we start by fitting a PLS-DA model with 2 components and with unit-variance (UV) scaling. The choice of components to use in the modeling will be addressed properly in the next section, the objective of this first section is to introduce the model syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to PCA, we start by choosing a scaling method for the X data matrix. The choice of scaling method will influence the results and interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the scaling options: \n",
    "\n",
    "# Unit-Variance (UV) scaling:\n",
    "scaling_object_uv = ChemometricsScaler(scale_power=1)\n",
    "\n",
    "# Pareto scaling:\n",
    "scaling_object_par = ChemometricsScaler(scale_power=1/2)\n",
    "\n",
    "# Mean Centring:\n",
    "scaling_object_mc = ChemometricsScaler(scale_power=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we will use Log(x+1) transformation and Mean Centring, start by fitting a PLS-DA model with 2 components. Feel free to explore other types of scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# # Run this cell to fit a PLS-DA model with 2 components and UV scaling\n",
    "# pls_da = ChemometricsPLSDA(n_components=2, x_scaler=scaling_object_uv)\n",
    "# pls_da.fit(X_train, y_train)\n",
    "# x_test_log = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log scale the data \n",
    "x_train_log = np.log1p(X_train)\n",
    "x_test_log = np.log1p(X_test)\n",
    "# Create and fit PLS-DA model\n",
    "pls_da = ChemometricsPLSDA(n_components=2, x_scaler=scaling_object_mc)\n",
    "pls_da.fit(x_train_log, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLS models perform dimensionality reduction in a manner similar to PCA. The main difference (besides the criteria in which the components are found) is that as well as the projections for the X matrix ($T$ scores) we also have projections for the Y matrix ($U$ scores).\n",
    "\n",
    "Model visualization of PLS/PLS-DA models is typically performed by plotting the $T$ scores (X matrix scores). \n",
    "The score plot gives an overview of the relationships between samples, their similarities and dissimilatrities within the model space.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Warning**: PLS-DA models can easily overfit, and the degree of separation or clustering of samples from distinct classes or Y outcome in the score plot is not a reliable measure of model validity. We recommend focusing on model validation before exploring the relationships in the scores plot. See the machine learning metrics and the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The *plot_scores* methods from `ChemometricsPLS` and `ChemometricsPLSDA` objects share the same functionality as `ChemometricsPCA.plot_scores`. Score plot data points can be colored by levels of a continuous or discrete covariate by using the `color` argument, and setting the ```discrete``` argument to ```True``` or ```False```, accordingly). The index (row index of the data matrix **X**) of the outlying can be labeled with ```label_outliers=True``` and the plot title changed with the argument```plot_title```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the scores\n",
    "pls_da.plot_scores(color=y_train, discrete=True, label_outliers=True, plot_title=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "It is also possible to assess the overfitting and general model performance by using \"machine learning\" metrics such as accuracy, precision, recall, f1, ROC curves and their respective area under the curve (AUC). These metrics are calculated by comparing the predicted Y values from the model with the true Y values. \n",
    "\n",
    "Below, you can see the confusion matrix for the testing dataset. The confusion matrix is a table with the number of true positives (TP), true negatives (TN), false positives (FP) and false negatives (FN). TP are values correctly predicted as positive, TN are values correctly predicted as negative, FP are values incorrectly predicted as positive and FN are values incorrectly predicted as negative. Using these values, we can calculate the accuracy, precision, recall, specificity and f1 score.\n",
    "\n",
    "$$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "$$Precision = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "$$Recall (Sensitivity) = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "$$Specificity = \\frac{TN}{TN + FP}$$\n",
    "\n",
    "$$F1 = 2 * \\frac{Precision * Recall}{Precision + Recall}$$\n",
    "\n",
    "AUC is the area under the ROC curve. The ROC curve is a plot of the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The TPR is the same as the recall, and the FPR is 1 - specificity. The AUC is a measure of how well a model can distinguish between classes. An AUC of 1 means the model can perfectly distinguish between classes, and an AUC of 0.5 means the model cannot distinguish between classes at all.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Predict the response Y (sex) based on the test set\n",
    "y_pred = pls_da.predict(x_test_log)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "# Compare the predicted Y values with the true Y values\n",
    "cm2 = confusion_matrix(y_test, np.where(y_pred > 0.5, 1, 0))\n",
    "disp2 = ConfusionMatrixDisplay(confusion_matrix=cm2, display_labels=[\"Female\", \"Male\"])\n",
    "disp2.plot()\n",
    "plt.show()\n",
    "\n",
    "# plot the metrics results\n",
    "print('Accuracy', round(accuracy_score(y_test, np.where(y_pred > 0.5, 1, 0)), 3))\n",
    "print('Precision', round(precision_score(y_test, np.where(y_pred > 0.5, 1, 0)), 3))\n",
    "print('Recall', round(recall_score(y_test, np.where(y_pred > 0.5, 1, 0)), 3))\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, np.where(y_pred > 0.5, 1, 0)).ravel()\n",
    "print('Specificity', round(tn/(tn+fp), 3))\n",
    "print('F1 Score', round(f1_score(y_test, np.where(y_pred > 0.5, 1, 0)), 3))\n",
    "print('AUC', round(roc_auc_score(y_test, np.where(y_pred > 0.5, 1, 0)), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main directions associated with each component in the score plots can be interpreted in terms of the original X variables using the loading vector, just like in PCA. Each component has an associated loading vector $p$ and weight vector $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The remaining plots with the new features I added into the pyChemometrics - flsoares232\n",
    "# Plot the weights\n",
    "pls_da.plot_model_parameters(parameter='w', component=1, instrument = 'lcms', xaxis=retention_times, yaxis=mz_values)\n",
    "\n",
    "# Plot the loadings\n",
    "pls_da.plot_model_parameters(parameter='p', component=1, instrument = 'lcms', xaxis=retention_times, yaxis=mz_values)\n",
    "\n",
    "# Plot beta\n",
    "pls_da.plot_model_parameters(parameter='beta', instrument = 'lcms', xaxis=retention_times, yaxis=mz_values)\n",
    "\n",
    "# Plot VIP\n",
    "pls_da.plot_model_parameters(parameter='VIP', instrument = 'lcms', xaxis=retention_times, yaxis=mz_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the weights and loadings.\n",
    "# w for weights, p for loadings,\n",
    "# ws for X rotations (rotated version of w)\n",
    "pls_da.plot_model_parameters(parameter='p', component=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the loading vectors, PLS models have another important set of parameters, the weight vectors. There is one weight vector ($w$) corresponding to the X matrix and another ($c$) to the Y variables.\n",
    "\n",
    "The weight vector ($w$) relates the original X variables with the Y outcome we are predicting. These vectors (and metrics based on them, such as VIP) are important to assess the relationship between X and Y and which X variables are more associated with Y. This will be discussed in more detail later in this tutorial.\n",
    "\n",
    "The larger the magnitude of the variable coefficient in the weight vector, the more \"associated\" that variable is with the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the weights and loadings.\n",
    "# w for weights, p for loadings,\n",
    "# ws for X rotations (rotated version of w) \n",
    "pls_da.plot_model_parameters(parameter='w', component=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the loading and weight vectors as a colourscale over the median spectrum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Model Selection - Number of components\n",
    "\n",
    "Selection of the number of components for a PLS model follows a very similar logic to the PCA case.\n",
    "Since the goal is to predict the Y variable, the main criteria used are the $R^{2}Y$/$Q^{2}Y$ as opposed to $R^{2}X$/$Q^{2}X$.\n",
    "\n",
    "Ideally, we want to select enough components to predict as much of the variation in Y as possible using the data in X, while avoiding overfitting. \n",
    "\n",
    "We apply a similar criterion as the one used with PCA: choosing as the number of components after which the $Q^{2}Y$ value reaches a plateau (less than 5% increase compared to previous number of components). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pls_da.scree_plot(x_train_log, y_train, total_comps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in the case of PCA, the $Q^{2}Y$ and other validation metrics obtained during K-Fold cross validation is sensitive to row permutation of the X and Y matrices. Shuffling the rows and repeating the cross-validation steps multiple times is a more reliable way to select the number of components.\n",
    "\n",
    "**Note**: Model cross-validation, especially the *repeated_cv* call in the next cell requires fitting the model multiple times. Do no run in Google Colab as it takes up to 25 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeated cross_validation - DO NOT RUN IN COLAB\n",
    "rep_cv = pls_da.repeated_cv(x_train_log, y_train, repeats=5, total_comps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier detection\n",
    "\n",
    "The outlier detection measures available for PCA (Hotelling $T^{2}$ and DmodX) are also available for PLS/PLS-DA models. Outlier interpretation is also performed in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls_da.plot_scores(label_outliers=True)\n",
    "pca_outliers = pls_da.outlier(x_train_log)\n",
    "print('Outliers: {0}'.format(pca_outliers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strongest outliers in this case are the 5 samples with more negative PLS component 2 scores. These are actually the same samples identified as outliers during the preliminary PCA analysis. We will remove them before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_no_outliers = np.delete(x_train_log, pca_outliers, axis=0)\n",
    "y_no_outliers = np.delete(y_train, pca_outliers, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now re-check the optimal number of components after exclusion of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls_da.scree_plot(x_no_outliers, y_no_outliers, total_comps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Repeated cross_validation - DO NOT RUN IN COLAB\n",
    "# rep_cv = pls_da.repeated_cv(x_no_outliers, y_no_outliers, repeats=6, total_comps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the recomendations from cross-validation and repeated cross validation we select 4 as the final number of components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refit the model\n",
    "Refit the model without outliers and use the number of components selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refit the model with the selected number of components\n",
    "pls_da = ChemometricsPLSDA(n_components=6, x_scaler=scaling_object_mc)\n",
    "pls_da.fit(x_no_outliers, y_no_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls_da.plot_scores(color=y_no_outliers, discrete=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we used the $Q^{2}Y$ metric to perform model selection, this metric is easier to interpret for regression problems, and it is not straightforward to assess the performance of a classifier model using $Q^{2}Y$ or $R^{2}Y$ and similar goodness of fit metrics. The performance in a classification task is more effectively described by confusion matrices and related metrics, such as accuracy/balanced accuracy, f1, ROC curves and their respective area under the curve.\n",
    "\n",
    "To obtain more reliable estimates we can calculate the cross-validation estimates of any of these metrics, including cross-validated ROC curves. This ROC curve was estimated using the left-out samples (the test sets) during cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validated ROC curve\n",
    "pls_da.cross_validation(x_no_outliers, y_no_outliers)\n",
    "pls_da.plot_cv_ROC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now we can assess the model performance using test set samples which were not used during model fitting or cross-validation. Compare the confusion matrices and classification metrics of the model fitted with 2 latent variables and the model fitted with 4 latent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Predict the response Y (sex) based on the test set\n",
    "y_pred = pls_da.predict(x_test_log)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "# Compare the predicted Y values with the true Y values\n",
    "cm2 = confusion_matrix(y_test, np.where(y_pred > 0.5, 1, 0))\n",
    "disp2 = ConfusionMatrixDisplay(confusion_matrix=cm2, display_labels=[\"Female\", \"Male\"])\n",
    "disp2.plot()\n",
    "plt.show()\n",
    "\n",
    "# plot the metrics results\n",
    "print('Accuracy', round(accuracy_score(y_test, np.where(y_pred > 0.5, 1, 0)), 3))\n",
    "print('Precision', round(precision_score(y_test, np.where(y_pred > 0.5, 1, 0)), 3))\n",
    "print('Recall', round(recall_score(y_test, np.where(y_pred > 0.5, 1, 0)), 3))\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, np.where(y_pred > 0.5, 1, 0)).ravel()\n",
    "print('Specificity', round(tn/(tn+fp), 3))\n",
    "print('F1 Score', round(f1_score(y_test, np.where(y_pred > 0.5, 1, 0)), 3))\n",
    "print('AUC', round(roc_auc_score(y_test, np.where(y_pred > 0.5, 1, 0)), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation Testing\n",
    "A final and very important method for model validation is the permutation randomization test. In a permutation randomisation test, the model will be refitted and assessed multiple times, but each time with the Y randomly permuted to destroy any relationship between X & Y. This allows us to assess what sort of model we can get when there really is no relationship between the two data matrices, and calculate the likelihood of obtaining a model with predictive performance as good as the non-permuted model by chance alone.\n",
    "\n",
    "During this test, the number of components, scaling, type of cross-validation employed, and any other modeling choice is kept constant. In each randomization, the model is refitted, and the AUC, $Q^{2}Y$ or any other validation metric is recorded. This enables the generation of permuted null distributions for any parameter, which can be used to obtain an empirical *p-value* for their significance.\n",
    "\n",
    "**Note** Running the permutation test with a large number of permutation randomizations (for example, 1000) is expected to take a considerable ammount of time (approximately 30 mins on a laptop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permt = pls_da.permutation_test(x_no_outliers, y_no_outliers, 1000)\n",
    "# np.save('./data/permutations_plsda.npy', permt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Load pre-calculated results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from Google Drive using bash command\n",
    "!wget \"https://drive.google.com/u/0/uc?id=1VFQqIpc0w6QaEgXaFKNKXE-QipnLHMJp&export=download&confirm=t&uuid=d1c8bfb9-3f76-4a85-a931-8d0785a8aecd&at=ALt4Tm1Zil7479CL_Xq3Anjkij7S:1689083835165\" -O \"permutations_plsda.npy\"\n",
    "\n",
    "permt = np.load('permutations_plsda.npy', allow_pickle=True)\n",
    "\n",
    "# Load form locally saved file\n",
    "# permt = np.load('./../../Data/IPTC/permutations_plsda.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results from the permuation test\n",
    "pls_da.plot_permutation_test(permt, metric='AUC')\n",
    "\n",
    "\n",
    "print(\"Permutation p-value for the AUC: {0}\".format(permt[1]['AUC']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results from the permuation test\n",
    "pls_da.plot_permutation_test(permt, metric='Q2Y')\n",
    "plt.xlabel('Q2Y')\n",
    "plt.ylabel('Counts')\n",
    "print(\"Permutation p-value for the Q2Y: {0}\".format(permt[1]['Q2Y']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *p-value* obtained is < 0.05, so the model AUC and Q2Y values are significantly different from what is expected by chance alone at a level of $\\alpha$ = 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Model interpretation and variable importance\n",
    "\n",
    "The main parameters to assess in terms of variable importance for the prediction of Y from X are the weights ($w$), the VIP metric and regression coefficients.\n",
    "\n",
    "The values in a weight vector vary between -1 (strong negative-covariance) and 1 (strong covariance), with 0 meaning no association/covariance. The weight vector of the first component (which explains the most variation in Y) is the primary weight vector to analyze when interpreting the main variables of X associed with Y.\n",
    "\n",
    "The variable importance for prediction (VIP) metric is a sum (weighted by the ammount of variance of Y explained by each respective component) of the squared weight values. It provides a summary of the importance of a variable accounting for all weight vectors. VIPs are bounded between 0 (no effect) and infinity. Because it is calculated from the weights $w$, for PLS models with a single component these are directly proportional to the $w^{2}$. The VIP metric has the disadvantage of pooling together $w$ vectors from components which contribute a very small magnitude to the model's $R^{2}Y$.\n",
    "\n",
    "The regression coefficients ($\\beta$) have a similar interpretation as regression coefficients in a multivariate/multiple linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this part, I don't know how you want to handle... because once the model_parameters are a 2D scatter now, we cannot see the data variantion\n",
    "# during permutation. So it won't be different from the plots a few cells above\n",
    "\n",
    "# But I've added this plot of VIP which I think it will be interesting to discuss with then\n",
    "\n",
    "pls_da.VIP_variableselection(x_no_outliers, threshold='knee', instrument='lcms',\n",
    "                                     xaxis=retention_times, yaxis=mz_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls_da.plot_model_parameters('w', component=1, sigma=2, cross_val=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls_da.plot_model_parameters('VIP', sigma=2, cross_val=True)\n",
    "# plt.gca().invert_xaxis()\n",
    "# plt.gca().set_xlabel('ppm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls_da.plot_model_parameters('beta', sigma=2, cross_val=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, assessment of variable importance in PLS-DA/PLS multivariate models is not straightfoward, given the multiple choice of parameters and their different interpretation, especially in models with more than 1 PLS component. To obtain a ranking of variables from the data matrix X associated with Y, we recommend starting with the weights $w$ of the first component, which contributes the most to $R^{2}Y$. \n",
    "\n",
    "However, it must be mentioned that the weights of the first PLS component are equal to the normalized (so that the weight vector has norm equal to 1) vector of the univariate covariances estimated between each X column or variable, and the Y vector. This implies there is no advantage in using a PLS model and $w$ when compared to a series of univariate analyses for variable ranking and selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(8, 5))\n",
    "X_scaled = pls_da.x_scaler.transform(x_no_outliers)\n",
    "\n",
    "cov_x_y = np.dot(y_no_outliers.T - y_no_outliers.mean(), X_scaled) / (y_no_outliers.shape[0]-1)\n",
    "cov_x_y = cov_x_y/np.linalg.norm(cov_x_y)\n",
    "\n",
    "ax[0].plot(cov_x_y, 'orange')\n",
    "ax[1].plot(pls_da.weights_w[:, 0], 'green')\n",
    "ax[0].set_xlabel('Normalised $Cov(X_{i}, Y)$')\n",
    "ax[1].set_xlabel('$w$ for PLS component 1')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another set of quantities which can be used to assess variable importance are the $\\beta$ regression coefficients. However, as with other multivariate regression models, the final $\\beta$ vector encodes information about the correlation structure of X and how it relates to Y, and the magnitude and sign of $\\beta$ coefficient express how to derive a \"good\" prediction of Y using X. Taking the magnitude of each $\\beta$ and using it to rank variables can be misleading.\n",
    "\n",
    "This does not mean necessarily that PLS should only be used as a predictive \"black box\" regressor/classifier and model interpretation avoided altogether. The strength of PLS for exploratory data analysis and interpretation resides on the latent variable projections. The scores $T$ or $U$ can be plotted and associated with other metadata variables, or even correlated or regressed against them, and the corresponding loading $p$ can be visualized to assess the signals which make up the latent variable signature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if we inspect the scores plot for components 2 and 3 it becomes apparent that although we have not added information about the Age covariate to the model, the PLS component number 3 seems to be associated with it. This hints that this component is accounting for some of the variability related with Age to improve the prediction. The loadings of this component can then be used to visualize which regions of the spectrum are correlated. \n",
    "\n",
    "**Note**: We recommend refering to loadings $p$ and not weights $w$ when interpreting latent variable signatures, especially in PLS components after the 1st component.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthogonal PLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The orthogonal PLS modeling technique can be used to assist intepretation of PLS latent variables.\n",
    "After obtaining a reliable PLS model, we generate an Orthogonal PLS/PLS-DA model with the same number of components as the PLS model. In an orthogonal PLS model, the first component is called predictive, and the subsequent components \"orthogonal\" because they are uncorrelated to the response Y. Compared to the equivalent PLS model, Orthogonal PLS models shuffle away variation from the loading vector $p$ of the first component to subsequent components, which can aid in interpretation of the latent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an Orthogonal PLS-DA version of the PLS-DA model fitted\n",
    "orthogonal_pls_da = ChemometricsOrthogonalPLSDA( x_scaler=scaling_object_mc, n_components=6)\n",
    "orthogonal_pls_da.fit(x_no_outliers, y_no_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Orthogonal PLS model we just fitted has 1 predictive component and 4 orthogonal components. The predictive component encodes the information in X directly associated with Y. \n",
    "The orthogonal components can be investigated and associated with other known covariates, to assist in understanding the sources of variation that the PLS model/Orthogonal PLS model is \"learning\" from the data to improve the prediction of Y (measured by the $R^{2}Y$)\n",
    "\n",
    "In the following plot, we investigate the scores on the predictive ($T_{pred}$) and first orthogonal component ($T_{ortho[1]}$), coloured by genotype.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orthogonal_pls_da.plot_scores(color=y_no_outliers, orthogonal_component=1, discrete=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation of the Orthogonal PLS score plorts model should be made using the predictive and orthogonal loading vectors ($p$) for all components. Only the weight vector $w$ for the predictive component should be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the weights and loadings - The updated version\n",
    "orthogonal_pls_da.plot_model_parameters('p_pred', orthogonal_component = 1, instrument = 'lcms', xaxis=retention_times, yaxis=mz_values)\n",
    "\n",
    "orthogonal_pls_da.plot_model_parameters('p_ortho', orthogonal_component = 2, instrument = 'lcms', xaxis=retention_times, yaxis=mz_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orthogonal_pls_da.plot_model_parameters('p_pred', orthogonal_component = 1)\n",
    "\n",
    "orthogonal_pls_da.plot_model_parameters('p_ortho', orthogonal_component = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also, here is a few aditional plots I've created that could be interesting to discuss\n",
    "# Variable Selection session - We need this to save extra outputs from CV\n",
    "pls_da.cross_validation(x_no_outliers, y_no_outliers, outputdist=True)\n",
    "\n",
    "pls_da.confusionmatrix_show (dataset='calibration')\n",
    "pls_da.confusionmatrix_show (dataset='cv')\n",
    "\n",
    "pls_da.plot_truevspredicted(x_no_outliers , y_no_outliers, dataset = 'calibration', color = y_no_outliers)\n",
    "pls_da.plot_truevspredicted(x_no_outliers , y_no_outliers, dataset = 'cv', color = y_no_outliers)\n",
    "\n",
    "pls_da.plot_missclassfsamples(y_no_outliers, dataset = 'calibration', color = y_no_outliers)\n",
    "pls_da.plot_missclassfsamples(y_no_outliers, dataset = 'cv', color = y_no_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation p-values for variable ranking\n",
    "\n",
    "The permutation test we ran before is also useful to obtain permuted null distributions for most of the model parameters. These can be used to obtain empirical confidence intervals and potentially permutation *p-values* for hypothesis testing.\n",
    "\n",
    "To illustrate this, the next cells generate histograms for the permuted distribution of the $w$ and $p$ for the first PLS component and regression coefficients for 2 randomly selected variables.\n",
    "Notice the differences between the permuted null distributions of weights, loadings and regression coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot empirical null distributions for weights\n",
    "plt.figure()\n",
    "plt.hist(permt[0]['Weights_w'][:, 3000, 0], 100)\n",
    "plt.title(\"Permuted null distribution for weights (w), component 1, {0}\".format(variable_names[3000]))\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(permt[0]['Weights_w'][:, 10, 0], 100)\n",
    "plt.title(\"Permuted null distribution for weights (w), component 1, {0} \".format(variable_names[10]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot empirical null distributions for loadings\n",
    "# Notice how these are not unimodal and distributed around 0...\n",
    "plt.figure()\n",
    "plt.hist(permt[0]['Loadings_p'][:, 3000, 0], 100)\n",
    "plt.title(\"Permuted null distribution for loadings (p), component 1, {0}\".format(variable_names[3000]))\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(permt[0]['Loadings_p'][:, 10, 0], 100)\n",
    "plt.title(\"Permuted null distribution for loadings (p), component 1, {0} \".format(variable_names[10]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot empirical null distributions for regression coefficients\n",
    "plt.figure()\n",
    "plt.hist(permt[0][\"Beta\"][:, 3000], 100)\n",
    "plt.title(r\"Permuted null distribution for $\\beta$, {0} \".format(variable_names[3000]))\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(permt[0]['Beta'][:, 10], 100)\n",
    "plt.title(r\"Permuted null distribution for $\\beta$, {0} \".format(variable_names[10]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the regression coefficients and weights have a null distribution centered around 0. Conversely, for the loadings, the center of the distribution is shifted. Loadings encode information about the variance and covariance (with the latent variable score) of each variable, and their magnitude is harder to interpret in terms of importance for prediction. The permutation performed in this manner does not change the correlation between variables in X, and therefore is not adequate to obtain permuted null distributions of the loading parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now calculate empirical p-values for the regression coefficients..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always set *nperms* equal to the number of permutations used before\n",
    "nperms = permt[0]['R2Y'].size\n",
    "perm_indx = abs(permt[0]['Beta'].squeeze()) >= abs(pls_da.beta_coeffs.squeeze())\n",
    "counts = np.sum(perm_indx, axis=0)\n",
    "beta_pvals = (counts + 1) / (nperms + 1)\n",
    "\n",
    "perm_indx_W = abs(permt[0]['Weights_w'][:, :, 0].squeeze()) >= abs(pls_da.weights_w[:, 0].squeeze())\n",
    "counts = np.sum(perm_indx_W, axis=0)\n",
    "w_pvals = (counts + 1) / (nperms + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(r\"p-value distribution for the regression coefficients $\\beta$ \")\n",
    "z = plt.hist(beta_pvals, bins=100, alpha=0.8)\n",
    "plt.axvline(x=0.05, ymin=0, ymax=max(z[0]), color='r', linestyle='--') \n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(r\"p-value distribution for the weights corresponding to the first component\")\n",
    "z = plt.hist(w_pvals, bins=100, alpha=0.8)\n",
    "plt.axvline(x=0.05, ymin=0, ymax=max(z[0]), color='r', linestyle='--') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and use the permutation test to obtain a list of statistically significant variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signif_bpls_idx = np.where(beta_pvals <= 0.05)[0]\n",
    "\n",
    "print(\"Number of significant values: {0}\".format(len(signif_bpls_idx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noting that a selection procedure of this kind is also a type of multiple testing, and it is recommended to apply false discovery rate or any other multiple testing correction to the *p-values* obtained in this manner. Also, formal inferential procedures to derive *p-values* and confidence intervals are not established for PLS models. Although *ad-hoc* solutions like a permutation test can be implemented as shown, some issues still remain - for example, the *p-value* distribution obtained for the regression coefficients is clearly non-uniform and care must be exercised when performing multiple testing correction or even interpreting the *p-values* obtained in this manner.\n",
    "The latent variable and dimensionality reduction provided by PLS/PLS-DA can be very usefull to visualize general trends in the data. However, interpreting which variables are important to the model and how they contribute for the explanation/separation between classes is not easy. We suggest complementing the inspection of multivariate model parameters with univariate analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
